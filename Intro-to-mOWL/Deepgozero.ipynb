{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# DeepGOZero\n\nThis example corresponds to the paper [DeepGOZero: improving protein function prediction from sequence and zero-shot learning based on ontology axioms](https://doi.org/10.1093/bioinformatics/btac256). DeepGOZero is a machine learning model that performs protein function prediction for functions that have small number or zero annotations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we have the necesary imports for this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import click as ck\nimport pandas as pd\nimport torch as th\nimport numpy as np\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch import optim\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom sklearn.metrics import roc_curve, auc\nimport math\nfrom mowl.utils.data import FastTensorDataLoader\nimport os\nimport pickle as pkl\nfrom tqdm import tqdm\n\nimport mowl\nmowl.init_jvm(\"10g\")\nfrom mowl.owlapi.defaults import BOT, TOP\nfrom mowl.datasets import ELDataset, RemoteDataset\nfrom mowl.nn import ELEmModule\nfrom mowl.owlapi import OWLAPIAdapter\nfrom mowl.datasets.base import Entities, OWLClasses, OWLIndividuals\n\nfrom org.semanticweb.owlapi.model import AxiomType\nfrom org.semanticweb.owlapi.model.parameters import Imports\nfrom org.semanticweb.owlapi.reasoner.structural import StructuralReasonerFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n\nThe datasets are stored in the cloud and the following links correspond for the data for the\nGene Ontology sub-ontologies: molecular function, biological process and cellular component.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MF_URL = \"https://deepgo.cbrc.kaust.edu.sa/data/deepgozero/mowl/molecular_function.tar.gz\"\nBP_URL = \"https://deepgo.cbrc.kaust.edu.sa/data/deepgozero/mowl/biological_process.tar.gz\"\nCC_URL = \"https://deepgo.cbrc.kaust.edu.sa/data/deepgozero/mowl/cellular_component.tar.gz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, each subontology data is encapsutaled in the ``DGZeroDataset``. This class contains \\\nthree ontologies: training, validation and testing.\nFor this project, the training ontology is the Gene Ontology extended with the following axioms:\n\n* $\\exists has\\_function. go\\_class (protein)$, which encodes protein function annotations.\n* $has\\_interpro (protein, interpro)$, which encodes interpro features for proteins.\n\nThe validation and testing ontologies contain protein function and intepro annotations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DGZeroDataset(RemoteDataset):\n    def __init__(self, subontology):\n        if subontology == \"mf\":\n            url = MF_URL\n            root = \"molecular_function/\"\n        elif subontology == \"bp\":\n            url = BP_URL\n            root = \"biological_process/\"\n        elif subontology == \"cc\":\n            url = CC_URL\n            root = \"cellular_component/\"\n        else:\n            raise ValueError(\"Invalid subontology: {}\".format(subontology))\n\n        train_owl_file = root + \"mowl_train.owl\"\n        valid_owl_file = root + \"mowl_valid.owl\"\n        test_owl_file = root + \"mowl_test.owl\"\n\n        super().__init__(url)\n        \n        self._proteins = None\n        self._functions = None\n        self._interpros = None\n        \n    @property\n    def functions(self):\n        if self._functions is None:\n            functions = set()\n            for cls_str, cls_owl in self.classes.as_dict.items():\n                if cls_str.startswith(\"http://purl.obolibrary.org/obo/GO\"):\n                    functions.add(cls_owl)\n            self._functions = OWLClasses(functions)\n        return self._functions\n\n    @property\n    def proteins(self):\n        if self._proteins is None:\n            proteins = set()\n            for ind_str, ind_owl in self.individuals.as_dict.items():\n                if ind_str.startswith(\"http://mowl/protein\"):\n                    proteins.add(ind_owl)\n            self._proteins = OWLIndividuals(proteins)\n        return self._proteins\n\n    @property\n    def interpros(self):\n        if self._interpros is None:\n            interpros = set()\n            for ind_str, ind_owl in self.individuals.as_dict.items():\n                if ind_str.startswith(\"http://mowl/interpro\"):\n                    interpros.add(ind_owl)\n            self._interpros = OWLIndividuals(interpros)\n        return self._interpros\n    \n\n    @property\n    def evaluation_property(self):\n        return \"http://mowl/has_function\"\n\n\n\ndef load_data(dataset, term_to_id, ipr_to_id):\n    train_data = get_data(dataset.ontology, term_to_id, ipr_to_id)\n    valid_data = get_data(dataset.validation, term_to_id, ipr_to_id)\n    test_data  = get_data(dataset.testing, term_to_id, ipr_to_id)\n    \n    return train_data, valid_data, test_data\n\ndef get_data(ontology, term_to_id, ipr_to_id):\n    axioms = ontology.getABoxAxioms(Imports.fromBoolean(False))\n    \n    pf_axioms = set()\n    interpro_axioms = set()\n    \n    for abox_axiom in axioms:\n        ax_name = abox_axiom.getAxiomType()\n        \n        if ax_name == AxiomType.CLASS_ASSERTION:\n            pf_axioms.add(abox_axiom)\n        elif ax_name == AxiomType.OBJECT_PROPERTY_ASSERTION:\n            interpro_axioms.add(abox_axiom)\n        else:\n            print(f\"Ignoring axiom: {abox_axiom.toString()}\")\n    \n    individuals = ontology.getIndividualsInSignature()\n    proteins = [str(i.toStringID()) for i in individuals if str(i.toStringID()).startswith(\"http://mowl/protein/\")]\n    proteins = sorted(proteins)\n    prot_to_id = {p: i for i, p in enumerate(proteins)}\n\n    data = th.zeros((len(proteins), len(ipr_to_id)), dtype=th.float32)\n    labels = th.zeros((len(proteins), len(term_to_id)), dtype=th.float32)\n    \n    interpro_count = 0\n    function_count = 0\n    for axiom in interpro_axioms:\n        protein = str(axiom.getSubject().toStringID())\n        interpro = str(axiom.getObject().toStringID())\n        \n        if interpro in ipr_to_id:\n            data[prot_to_id[protein], ipr_to_id[interpro]] = 1\n            interpro_count += 1\n\n    for axiom in pf_axioms:\n        protein = str(axiom.getIndividual().toStringID())\n        function = str(axiom.getClassExpression().getFiller().toStringID())\n        \n        if function in term_to_id:\n            labels[prot_to_id[protein], term_to_id[function]] = 1\n            function_count += 1\n    \n    print(f\"In get_data. Interpros processed: {interpro_count}. Functions processed: {function_count}\")\n    return data, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DeepGoZero model\n\nThe DeepGoZero model is composed by:\n- A protein encoder model that takes protein interpro features and learns a latent \\\nrepresentation of the protein. Futhermore, this representation is associated to a GO term \\\nto predict if the GO term is a function of the protein.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return x + self.fn(x)\n    \n        \nclass MLPBlock(nn.Module):\n\n    def __init__(self, in_features, out_features, bias=True, layer_norm=True, dropout=0.1, activation=nn.ReLU):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n        self.activation = activation()\n        self.layer_norm = nn.BatchNorm1d(out_features) if layer_norm else None\n        self.dropout = nn.Dropout(dropout) if dropout else None\n\n    def forward(self, x):\n        x = self.activation(self.linear(x))\n        if self.layer_norm:\n            x = self.layer_norm(x)\n        if self.dropout:\n            x = self.dropout(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GO terms representations are learned using a model theoretic approach called\n:doc:`ELEmbeddings </examples/elmodels/plot_1_elembeddings>`. ELEmbeddings processes the axioms\nof the Gene Ontology and learns a representation of the GO terms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DGELModel(nn.Module):\n\n    def __init__(self, nb_iprs, nb_gos, nb_zero_gos, nb_rels, device, hidden_dim=1024, embed_dim=1024, margin=0.1):\n        super().__init__()\n        self.nb_gos = nb_gos\n        self.nb_zero_gos = nb_zero_gos\n        input_length = nb_iprs\n        net = []\n        net.append(MLPBlock(input_length, hidden_dim))\n        net.append(Residual(MLPBlock(hidden_dim, hidden_dim)))\n        self.net = nn.Sequential(*net)\n\n        # ELEmbeddings\n        self.embed_dim = embed_dim\n        self.hasFuncIndex = th.LongTensor([nb_rels]).to(device)\n        go_embed = nn.Embedding(nb_gos + nb_zero_gos+2, embed_dim)\n        #self.go_norm = nn.BatchNorm1d(embed_dim)\n        k = math.sqrt(1 / embed_dim)\n        nn.init.uniform_(go_embed.weight, -k, k)\n        go_rad = nn.Embedding(nb_gos + nb_zero_gos, 1)\n        nn.init.uniform_(go_rad.weight, -k, k)\n        \n        rel_embed = nn.Embedding(nb_rels + 1, embed_dim)\n        nn.init.uniform_(rel_embed.weight, -k, k)\n        self.all_gos = th.arange(self.nb_gos).to(device)\n        self.margin = margin\n\n        self.elembeddings = ELEmModule(nb_gos + nb_zero_gos + 2, nb_rels+1, embed_dim=embed_dim) # +2 to add top and bottom\n        self.elembeddings.class_embed = go_embed\n        self.elembeddings.class_rad = go_rad\n        self.elembeddings.rel_embed = rel_embed\n        \n     \n    def forward(self, features, data = None):\n        if data is None:\n            data = self.all_gos\n\n        class_embed = self.elembeddings.class_embed\n        rel_embed = self.elembeddings.rel_embed\n        class_rad = self.elembeddings.class_rad\n        x = self.net(features)\n        go_embed = class_embed(data)\n        hasFunc = rel_embed(self.hasFuncIndex)\n        hasFuncGO = go_embed + hasFunc\n        go_rad = th.abs(class_rad(data).view(1, -1))\n        x = th.matmul(x, hasFuncGO.T) + go_rad\n        logits = th.sigmoid(x)\n        return logits\n\n    def predict_zero(self, features, data):\n        return self.forward(features, data=data)\n    \n    def el_loss(self, go_normal_forms):\n        gci0, gci1, gci2, gci3 = go_normal_forms\n        \n        gci0_loss = self.elembeddings(gci0, \"gci0\")\n        gci1_loss = self.elembeddings(gci1, \"gci1\")\n        gci2_loss = self.elembeddings(gci2, \"gci2\")\n        gci3_loss = self.elembeddings(gci3, \"gci3\")\n        return gci0_loss.mean() + gci1_loss.mean() + gci2_loss.mean() + gci3_loss.mean()\n\n    \n\ndef compute_roc(labels, preds):\n    # Compute ROC curve and ROC area for each class\n    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n    roc_auc = auc(fpr, tpr)\n\n    return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training DeepGoZero\n\nIn the training phase, both the protein and GO term model are trained jointly. In the model, the\nobjective function is composed by two terms:\n- The first term is the cross entropy loss between the predicted GO term and the true GO term\nfor a protein\n- The second term is the ELEmbeddings loss that is computed using the axioms of the Gene Ontology\n\nNot all the GO terms are present in the first component, but only on the second component.\nHowever, DeepGOZero is able to predict protein functions that do not have annotations by\nleveraging the semantics of the Gene Ontology.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def main(ont, batch_size, epochs, device):\n\n    if not os.path.exists(f\"data/{ont}\"):\n        os.makedirs(f\"data/{ont}\")\n    \n    print(\"Loading DeepGOZero dataset...\")\n    dataset = DGZeroDataset(ont)\n    \n    model_file = f'data/{ont}/deepgozero_zero_10.th'\n    terms_file = str(dataset.root) + '/terms_zero_10.pkl'\n    iprs_file = str(dataset.root) + '/interpros.pkl'\n    out_file = str(dataset.root) + '/predictions_deepgozero_zero_10.pkl'\n\n    functions = dataset.functions.as_str\n    function_to_id = {f: i for i,f in enumerate(functions)}\n\n    proteins = dataset.proteins.as_str\n    protein_to_id = {p: i for i, p in enumerate(proteins)}\n\n    interpros = dataset.interpros.as_str\n    interpro_to_id = {ip: i for i, ip in enumerate(interpros)}\n\n    relations = dataset.object_properties.as_str\n    relation_to_id = {r: i for i, r in enumerate(relations) if r != \"http://mowl/has_function\"}\n\n    print(f\"Functions:\\t{len(functions)}\")\n    print(f\"Proteins: \\t{len(proteins)}\")\n    print(f\"Interpros:\\t{len(interpros)}\")\n    print(f\"Relations:\\t{len(relations)}\")\n\n\n    # List of GO terms to be used\n    terms_df = pd.read_pickle(terms_file)\n    terms = terms_df['gos'].values.flatten()\n    terms = [\"http://purl.obolibrary.org/obo/\" + t.replace(\":\", \"_\") for t in terms]\n    term_to_id = {t: i for i, t in enumerate(terms)}\n    n_terms = len(terms)\n    \n    # List of Interpros to be used\n    ipr_df = pd.read_pickle(iprs_file)\n    iprs = ipr_df['interpros'].values.flatten()\n    iprs = [\"http://mowl/interpro/\" + i for i in iprs]\n    ipr_to_id = {v:k for k, v in enumerate(iprs)}\n    n_interpros = len(iprs)\n    \n    print(f\"GO terms list: {n_terms}\")\n    print(f\"Interpro list: {n_interpros}\")\n\n\n    z_count = 0\n    z_functions = set()\n    for function in functions:\n        if not function in terms:\n            z_functions.add(function)\n            z_count += 1\n\n    print(f'Non-zero functions:\\t{n_terms}\\nZero functions: \\t{z_count}')\n\n \n\n    zero_functions = {t: i + len(terms) for i, t in enumerate(z_functions)}\n    class_to_id = {**term_to_id,  **zero_functions}\n    class_to_id[BOT] = len(class_to_id)\n    class_to_id[TOP] = len(class_to_id)\n\n    # Protein function data\n    train_data, valid_data, test_data = load_data(dataset, term_to_id, ipr_to_id)\n\n    # GO data as EL\n    nfs_file = f\"data/{ont}/nfs.pkl\"\n    if os.path.exists(nfs_file):\n        print(\"Loading normal forms from disk...\")\n        with open(nfs_file, \"rb\") as f:\n            nfs = pkl.load(f)\n            gci0_ds, gci1_ds, gci2_ds, gci3_ds = nfs\n    else:\n        print(\"Generating EL dataset...\")\n        el_dataset = ELDataset(dataset.ontology, \n                               class_index_dict=class_to_id,\n                               object_property_index_dict=relation_to_id, \n                               extended=False)\n\n        nfs = el_dataset.get_gci_datasets()    \n        with open(nfs_file, \"wb\") as f:\n            pkl.dump(nfs, f)\n\n    gci0_ds = nfs[\"gci0\"]\n    gci1_ds = nfs[\"gci1\"]\n    gci2_ds = nfs[\"gci2\"]\n    gci3_ds = nfs[\"gci3\"]\n    print(f\"Axioms in GCI0: {len(gci0_ds)}\")\n    print(f\"Axioms in GCI1: {len(gci1_ds)}\")\n    print(f\"Axioms in GCI2: {len(gci2_ds)}\")\n    print(f\"Axioms in GCI3: {len(gci3_ds)}\")\n\n    nfs = list(nfs.values())\n\n    n_rels = len(relation_to_id)\n    n_zeros = len(zero_functions)\n\n    net = DGELModel(n_interpros, n_terms, n_zeros, n_rels, device).to(device)\n    print(net)\n\n    train_features, train_labels = train_data\n    valid_features, valid_labels = valid_data\n    test_features, test_labels = test_data\n\n    train_loader = FastTensorDataLoader(\n        *train_data, batch_size=batch_size, shuffle=True)\n    valid_loader = FastTensorDataLoader(\n        *valid_data, batch_size=batch_size, shuffle=False)\n    test_loader = FastTensorDataLoader(\n        *test_data, batch_size=batch_size, shuffle=False)\n\n    valid_labels = valid_labels.detach().cpu().numpy()\n    test_labels = test_labels.detach().cpu().numpy()\n\n    optimizer = th.optim.Adam(net.parameters(), lr=5e-4)\n    scheduler = MultiStepLR(optimizer, milestones=[5, 20], gamma=0.1)\n\n    best_loss = 10000.0\n    \n    print('Training the model')\n    for epoch in range(epochs):\n        net.train()\n        train_loss = 0\n        train_elloss = 0\n        lmbda = 0.1\n        train_steps = 2 # int(math.ceil(len(train_labels) / batch_size))\n\n        count = 0\n        for batch_features, batch_labels in tqdm(train_loader, total=train_steps):\n            if count == train_steps:\n                break\n            count += 1\n            batch_features = batch_features.to(device)\n            batch_labels = batch_labels.to(device)\n            logits = net(batch_features)\n            loss = F.binary_cross_entropy(logits, batch_labels)\n            el_loss = net.el_loss(nfs)\n            total_loss = loss + el_loss\n            train_loss += loss.detach().item()\n            train_elloss = el_loss.detach().item()\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n\n        train_loss /= train_steps\n\n        print('Validation')\n        net.eval()\n        with th.no_grad():\n            valid_steps = int(math.ceil(len(valid_labels) / batch_size))\n            valid_loss = 0\n            preds = []\n\n            for batch_features, batch_labels in tqdm(valid_loader, total=valid_steps):\n                batch_features = batch_features.to(device)\n                batch_labels = batch_labels.to(device)\n                logits = net(batch_features)\n                batch_loss = F.binary_cross_entropy(logits, batch_labels)\n                valid_loss += batch_loss.detach().item()\n                preds = np.append(preds, logits.detach().cpu().numpy())\n            valid_loss /= valid_steps\n            roc_auc = compute_roc(valid_labels, preds)\n            print(f'Epoch {epoch}: Loss - {train_loss}, EL Loss: {train_elloss}, Valid loss - {valid_loss}, AUC - {roc_auc}')\n\n        print('EL Loss', train_elloss)\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            print('Saving model')\n            th.save(net.state_dict(), model_file)\n\n        scheduler.step()\n\n\n    # Loading best model\n    print('Loading the best model')\n    net.load_state_dict(th.load(model_file))\n    net.eval()\n    with th.no_grad():\n        test_steps = int(math.ceil(len(test_labels) / batch_size))\n        test_loss = 0\n        preds = []\n        \n        for batch_features, batch_labels in tqdm(test_loader, total=test_steps):\n            batch_features = batch_features.to(device)\n            batch_labels = batch_labels.to(device)\n            logits = net(batch_features)\n            batch_loss = F.binary_cross_entropy(logits, batch_labels)\n            test_loss += batch_loss.detach().cpu().item()\n            preds = np.append(preds, logits.detach().cpu().numpy())\n        test_loss /= test_steps\n        preds = preds.reshape(-1, n_terms)\n        roc_auc = compute_roc(test_labels, preds)\n        print(f'Test Loss - {test_loss}, AUC - {roc_auc}')\n\n    preds = list(preds)\n\n\n    adapter = OWLAPIAdapter()\n    manager = adapter.owl_manager\n\n    # Propagate scores using ontology structure\n\n\n    reasoner = StructuralReasonerFactory().createReasoner(dataset.ontology)\n\n    \n\n    for i, scores in tqdm(enumerate(preds[:10]), total=len(preds[:10])):\n        prop_annots = {}\n        sup_processed = 0\n        for go_id, j in term_to_id.items():\n            score = scores[j]\n            go_class = adapter.create_class(go_id)\n            superclasses = reasoner.getSuperClasses(go_class, False).getFlattened()\n            superclasses = [str(sup.toStringID()) for sup in superclasses]\n            for sup_go in superclasses:\n                if sup_go in prop_annots:\n                    prop_annots[sup_go] = max(prop_annots[sup_go], score)\n                    sup_processed += 1\n                else:\n                    prop_annots[sup_go] = score\n        for go_id, score in prop_annots.items():\n            if go_id in term_to_id:\n                scores[term_to_id[go_id]] = score\n\n\n\n    # TODO: refactor this to save predictions in an .owl file\n    # test_df['preds'] = preds\n    # test_df.to_pickle(out_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ont = \"mf\"\nbatch_size = 16\nepochs = 20\ndevice = \"cpu\"\nmain(ont, batch_size, epochs, device)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}